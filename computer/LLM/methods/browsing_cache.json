{
    "https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/": {
        "content": "Title: NanoGPT Speedrun Living Worklog\n\nURL Source: https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/\n\nPublished Time: 2025-03-08T08:00:00.000Z\n\nMarkdown Content:\nHow fast can I train GPT-2 on two RTX 4090 GPUs?\n\nMarch 8, 2025\n\nI\u2019ve seen [some](https://x.com/kellerjordan0/status/1859331370268623321)[really](https://x.com/kellerjordan0/status/1842300916864844014)[awesome](https://x.com/kellerjordan0/status/1876048851158880624)[GPT-2](https://x.com/hi_tysam/status/1879687807678959729) speedrun results from people like [Keller Jordan](https://x.com/kellerjordan0), [Fern](https://x.com/hi_tysam), [Braden Koszarsky](https://x.com/KoszarskyB), and others. I got a little inspired and wanted to see how fast I could train GPT-2 on my own hardware.\n\nTechnically, [the NanoGPT speedrun](https://x.com/kellerjordan0/status/1798863559243513937) is to train a neural network to 3.28 validation loss on FineWeb as fast as possible on an 8xH100 node. [Keller Jordan maintains a leaderboard here](https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history). At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!).\n\nI have access to **2xRTX 4090 GPUs** and I want to see how fast I can train GPT-2 on them by following the same rules as the NanoGPT speedrun. If I see some success, I may try to transfer my methods to an 8xH100 node for comparison with the main leaderboard.\n\nI\u2019ll be documenting my progress here and updating this post as I go. Code can be found in [this GitHub repo](https://github.com/tyler-romero/nanogpt-speedrun).\n\nProgress so far\n---------------\n\n| # | Description | Record time | Training Tokens | Tokens/Second | Date | Commit | Log |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| [1](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#1-initial-setup-and-baseline) | Initial baseline | 8.13 hours | 6.44B | 221k | 2025/01/16 | [b3c32f8](https://github.com/tyler-romero/nanogpt-speedrun/commit/b3c32f8937c1f4655c5eb9607970e03e351a6c08) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/4c627c0d-029c-4f8a-bd18-40f99b43b22e.txt) |\n| [2.1](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#21-architectural-changes-and-training-tweaks) | Architectural changes | 7.51 hours | 5.07B | 188k | 2025/01/18 | [b7bb93f](https://github.com/tyler-romero/nanogpt-speedrun/commit/b7bb93fd988d73a55184c553f0020feec1454340) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/14fcdb07-443d-4d1c-b307-061bc4bd2cd6.txt) |\n| [2.2](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#22-muon-optimizer) | Muon optimizer | 4.53 hours | 3.04B | 187k | 2025/01/23 | [b91c2c0](https://github.com/tyler-romero/nanogpt-speedrun/commit/b91c2c00673b125944abde277dd5ef3dc141284d) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/59951c17-fbe5-4577-a1bc-6dc0c1802d2e.txt) |\n| [2.3](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#23-dataloading-tweaks) | Dataloading tweaks | 4.26 hours | 3.31B | 216k | 2025/02/18 | [d59944d](https://github.com/tyler-romero/nanogpt-speedrun/commit/d59944dbe8535fea8ea107d9a6fb133de5346de5) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/08047f73-cb01-4f47-a901-de901b2a6b6e.txt) |\n| [2.4](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#24-logit-soft-capping) | Logit Soft-capping at 30 | 4.01 hours | 3.15B | 218k | 2025/02/23 | [12eab44](https://github.com/tyler-romero/nanogpt-speedrun/commit/12eab44ca1bce8783a3b4d43bfef357eff1a652e) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/2dbf7fa6-561c-49bc-8aae-665fefdd9a44.txt) |\n| [3](https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/#3-longer-training-and-evaluation-sequence-length) | Longer Sequence Length | 2.55 hours | 1.88B | 205k | 2025/03/03 | [d982ed5](https://github.com/tyler-romero/nanogpt-speedrun/commit/d982ed5900922e43a266c5d671b88f36efe72aaf) | [here](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/cf1ef5f9-9f79-4798-9360-2b174d8eb25f.txt) |\n\n1. Initial setup and baseline\n-----------------------------\n\nPart of the goal of this project is for me to learn as I go, so I am going to start at the beginning - with with Andrej Karpathy\u2019s [PyTorch GPT-2 trainer](https://github.com/karpathy/llm.c/blob/7b929300217ff1a974b63791a228928b39b26409/train_gpt2.py) from [llm.c](https://github.com/karpathy/llm.c). This is the script that Keller Jordan used for [his initial baseline](https://github.com/KellerJordan/modded-nanogpt/tree/master?tab=readme-ov-file#modded-nanogpt). This trainer is very similar to the NanoGPT trainer with some minor modifications / simplifications (such as no dropout).\n\nI have upstreamed some QOL improvements and basic tweaks to the training script from Keller\u2019s fork, but have not changed any of the core training / modeling logic. Specifically:\n\n1.   Implemented gradient accumulation so that my 2x24GB GPUs simulate the training experience of a 8xH100 machine.\n2.   Increased learning rate to 0.0015 and halved the batch size (total batch size is 262144 - that is bs of `32/device * 2 devices * 1024 sequence length * 4 gradient accum steps`).\n3.   Improved learning rate schedule (linear warmup then linear decay).\n4.   Removed all affine scale/bias parameters and switched to RMSNorm.\n5.   Padded the vocab size from 50257 to 50304 to make it a multiple of 128 (for better tensor core utilization).\n6.   Using Pytorch 2.5.1 (the switch from 2.4 to 2.5 gave ~9% speedup on the 8xH100 leaderboard).\n\nAdditionally, I added `wandb` logging for easy tracking of training progress - optimistically I may need to remove this one day as it slightly increases step time.\n\nCommit with the initial setup is here: [`b3c32f8`](https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/4c627c0d-029c-4f8a-bd18-40f99b43b22e.txt).\n\nThe baseline run time on my 2xRTX 4090 setup is **8.13 hours**.\n\n2. Implementing major improvements from the 8xH100 leaderboard\n--------------------------------------------------------------\n\nWaiting 8 hours for a result is too slow for effective experimentation, so I\u2019m going to begin by implementing some of the notable improvements from the 8xH100 leaderboard. I\u2019ll start with the most impactful/easiest changes first:\n\n1.   Architectural changes and training tweaks\n2.   Muon optimizer\n3.   Dataloading tweaks\n4.   Logit Softcapping\n\n### 2.1 Architectural changes and training tweaks\n\nThere are some basic architectural changes and modernizations that can be made to the model that will speed up training. These changes are general improvements to the transformer decoder architecture that have been generally adopted since the original GPT-2 paper. The changes are:\n\n1.   [RoPE (Rotary Positional Embeddings)](https://arxiv.org/abs/2104.09864). There are [many](https://www.jitx.io/posts/rope-embeddings)[good](https://blog.eleuther.ai/rotary-embeddings/) explanations of RoPE out there so I won\u2019t go into detail here.\n2.   [ReLU^2 Activation](https://arxiv.org/pdf/2109.08668)ReLU^2 activation function. ![Image 1: Relu Activation plot](https://www.tylerromero.com/assets/img/Zn-fTGuzYQ-300.png). Many activations that are better than GeLU have been proposed since GPT-2. ReLU^2 is a simple one that has been shown to be effective in decreasing training time required to reach a certain validation loss.\n3.   No gradient clipping. Gradient clipping can help stabilize training but it also slows down training. Since we are speed-running, we will remove gradient clipping. This also eliminates a hyperparameter that needs to be tuned.\n4.   [Trapezoidal learning rate schedule](https://arxiv.org/abs/2405.18392). While cosine learning rate schedules are the de-facto standard, they can be difficult to work with since changing the number of training steps changes the entire schedule. Trapezoidal learning rate schedules are often easier to reason about / tune around, and they have been show to match the performance of cosine schedules.\n\nIn addition, learning rate and batch size have been tuned.\n\nOnce again, many of these changes are [downstreamed](https://en.wikipedia.org/wiki/Downstream_(software_development)) from the [modded-nanogpt](https://github.com/KellerJordan/modded-nanogpt) repository / 8xH100 speedrun. Its not efficient to reinvent the wheel, and I want to get training time down as fast as possible in the beginning.\n\nAfter implementing these changes (commit [`b7bb93f`](https://github.com/tyler-romero/nanogpt-speedrun/commit/b7bb93fd988d73a55184c553f0020feec1454340)), the new run time is **7.51 hours**. This run was more data-efficient than the baseline, requiring only 5.07B tokens. However, the tokens/second increased, likely due to the larger batch size (more gradient accumulation steps which tends to translate to lower throughput) and the architectural changes, such as the inclusion of RoPE. Once I have a shorter run time, I will be able to tune more effectively and see if I can remove gradient accumulation.\n\n![Image 2: Section 2.1 loss plot](https://www.tylerromero.com/assets/img/J9W8EvOzUU-300.png)\n\n### 2.2 Muon Optimizer\n\nThe [Muon Optimizer](https://kellerjordan.github.io/posts/muon/) is a new optimizer developed with and for the NanoGPT speedrun by Jordan et al. It is a variant of SGD with Momentum that applies a postprocessing step to the gradient updates to approximately orthogonalize each update matrix. Muon has [some](https://kellerjordan.github.io/posts/muon/#why-is-it-good-to-orthogonalize-the-update)[connections](https://x.com/leloykun/status/1846842883967692926) to approximate second-order optimizers But are these approximate second-order methods actually second-order? [New research](https://arxiv.org/abs/2409.20325v1) suggests that methods like Shampoo and Adam can be viewed as variants of steepest descent under specific norms, and thus are actually first-order methods. like [Shampoo](https://arxiv.org/abs/1802.09568).\n\nI highly recommend reading the original [Muon blog post](https://kellerjordan.github.io/posts/muon/) for more details, as well as checking out the optimizer comparison for GPT-2 speedrunning that Keller Jordan put to gether [here](https://github.com/KellerJordan/modded-nanogpt/tree/master/records/102924_Optimizers). For those interested in a more step-by-step walkthrough of Muon, check out [this excellent post](https://jeremybernste.in/writing/deriving-muon) by Jeremy Bernstein.\n\nMuon is designed to work on _Linear_ layers, so it is not quite a drop-in replacement for AdamW (e.g. it isn\u2019t meant to optimize Embedding layers). However it can be used to optimize all of the hidden layers of our GPT-2 model. The output `lm_head` layer and the token embeddings will still be optimized with AdamW.\n\nJust like on the 8xH100 leaderboard, we observe a massive speedup when switching to Muon. The new run time is **4.53 hours**, requiring only 3.04B tokens. The tokens/second is also very similar to the previous run, which is a good sign that we are not losing throughput by switching optimizers.\n\n![Image 3: Section 2.2 loss plot](https://www.tylerromero.com/assets/img/OLzSwEFmsl-300.png)\n\n### 2.3 Dataloading Tweaks\n\nAs we have improved our data efficiency via architecture tweaks and an optimizer change, our training throughput has dropped from 221k tokens/second to 187k tokens/second. That is a ~15% drop in throughput. Recovering most of that throughput could provide a significant improvement to our run time. An obvious place to start is with our dataloading and gradient accumulation logic.\n\nUp until now, we have loaded a full-batch of data on each device and then split that full batch into smaller chunks (micro-batches) for each gradient accumulation step (recall that we are doing 8 accumulation steps per gradient update). We can instead make a minor tweak to our logic to load only the next micro-batch at each step of the dataloader, and then step the dataloader for each gradient accumulation step.\n\nWe also increase our torch version from `2.5` to `2.6` (which was recently released), and, in accordance with the [new official rules](https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#timing-change-after-record-21) designated on 2025/02/01, we have removed the use of `torch._inductor.config.coordinate_descent_tuning`.\n\nThese tweak brings our throughput back up to 216k tokens/second. In order to make runs more consistently hit the 3.28 validation loss target Note that there is some variance in the amount of time it takes for a speedrun candidate to run. For a speedrun to be an official record, it must attain a _mean_ validation loss of less than 3.28. I have been a bit lax about this so far because the time difference between runs has been large, and variance relatively small., we have also slightly increased the total number of training steps, so now 3.31B tokens are consumed. The new run time is **4.26 hours**, and the changes can be found at [`d59944d`](https://github.com/tyler-romero/nanogpt-speedrun/commit/d59944dbe8535fea8ea107d9a6fb133de5346de5).\n\n![Image 4: Section 2.3 loss plot](https://www.tylerromero.com/assets/img/k6VycAKAlz-300.png)\n\nAt this point, we code that can train GPT-2 almost twice as fast as the baseline.\n\n### 2.4 Logit Soft-capping\n\nLogit soft-capping is a technique popularized by [Gemma 2](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf) and initially used to improve the NanoGPT speedrun by [@Grad62304977](https://x.com/Grad62304977).\n\nSoft-capping is essentially a smooth and differentiable version of clipping\u2295 Soft-capping vs Clipping at \u00b15: ![Image 5: Soft-capping](https://www.tylerromero.com/assets/img/NOzgbhQs-D-300.png):\n\nsoftcap(x,cap)=cap\u22c5tanh\u2061(x cap)\\text{softcap(x, cap)} = \\text{cap} \\cdot \\tanh\\left(\\frac{\\text{x}}{\\text{cap}}\\right)\n\nLogit soft-capping prevents logits from growing excessively large by scaling them to a fixed range, which seems to help improve training dynamics. One could argue that this is imposing an inductive bias - and since we\u2019re in a relatively small model/low data regime that this is helpful.\n\nAfter implementing logit soft-capping with a cap of 30 (and doing some learning-rate tuning), the new run time is **4.01 hours**, requiring 3.15B tokens (commit [`12eab44`](https://github.com/tyler-romero/nanogpt-speedrun/commit/12eab44ca1bce8783a3b4d43bfef357eff1a652e)). Throughput remained steady at ~218k tokens/second.\n\n![Image 6: Section 2.4 loss plot](https://www.tylerromero.com/assets/img/1PFph9-p9J-300.png)\n\n3 Longer Training and Evaluation Sequence Length\n------------------------------------------------\n\nSo far, we\u2019ve been training and evaluating on sequences of 1024 tokens. We also haven\u2019t been particularly clever about how those sequences are processed. At each step, we simply load the next 1024 tokens into an element of the batch without regard for where the document starts or stops. That means much of the time we are starting in the middle of a document and cutting that document off before it reaches its end. We are also attending to tokens _across documents_ since we\u2019re just using a simple causal mask.\n\nCutting off documents in the middle is an especially large issue. See this plot of average loss vs sequence position: ![Image 7: Average Loss vs Sequence Position](https://www.tylerromero.com/assets/img/P0eirmJcqD-300.png)\n\nNotice how the first twenty-five or so positions have a much higher average loss than the later positions. This is because at the beginning of the sequence the LLM has much less information with which to make informed predictions about the next token in the sequence. We want to avoid needlessly restarting documents/sequences in order to avoid this loss penalty!\n\nA natural question to ask at this point is: how long are sequences in our dataset, on average? ![Image 8: Sequence Length CDF Plot](https://www.tylerromero.com/assets/img/XTDofjqWvO-300.png)\n\nThe data reveals that approximately 20% of documents exceed our current 1024 token sequence length. By increasing the sequence length to >=8192 tokens, we can accommodate virtually all documents in our dataset without truncation.\n\nTo address the issues identified above, we\u2019ll implement two key improvements. First, we\u2019ll extend our sequence length to minimize document splitting across sequence boundaries. Taking this approach to its logical conclusion, we\u2019ll eliminate the traditional batch dimension entirely and instead maximize sequence length (effectively using a \u201cbatch size\u201d of 1 that contains multiple concatenated documents). Second, we\u2019ll implement sophisticated attention masking that prevents cross-document attention while simultaneously leveraging the computational efficiency of sparse attention patterns.\n\nFortunately, [FlexAttention](https://pytorch.org/blog/flexattention/) provides an elegant solution that maintains the performance benefits of [FlashAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention) while enabling these improvements. One of FlexAttention\u2019s primary strengths is its ability to efficiently handle sparse, custom attention masks, making it ideal for our use case.\n\nTo implement FlexAttention, we need to define an appropriate attention mask that handles our specific requirements:\n\n```\ndef make_attn_mask(idx, eot_token, window_size=1024):\n    # Create a causal mask (only attend to past tokens)\n    def causal_mask(b, h, q_idx, kv_idx):\n        return q_idx >= kv_idx\n\n    # Track document boundaries using end-of-text tokens\n    documents = (idx == eot_token).cumsum(dim=1)\n\n    # Only allow attention within the same document\n    def document_mask(b, h, q_idx, kv_idx):\n        return documents[b, q_idx] == documents[b, kv_idx]\n\n    # Limit attention to an N-token window for efficiency\n    def sliding_window_mask(b, h, q_idx, kv_idx):\n        return q_idx - kv_idx <= window_size\n\n    return and_masks(document_mask, causal_mask, sliding_window_mask)\n```\n\nLet\u2019s break down each mask:\n\n1.   **Causal Mask**: Standard in autoregressive language modeling. Ensures that tokens can only attend to previous tokens in the sequence, preventing information leakage from future tokens.\n\n2.   **Document Mask**: This restricts attention to tokens within the same document. By tracking document boundaries using end-of-text tokens, we prevent tokens from attending across different documents, which helps the model maintain coherent context within a single document.\n\n3.   **Sliding Window Mask**: This limits attention to a fixed window of tokens before the current position. This approach balances efficiency with context retention with a clear tradeoff: smaller windows are more efficient but may miss long-range dependencies, while larger windows capture more context at the expense of resources.\n\nIn order to build intuition about the individual component masks, we visualize them below: ![Image 9: Causal, Document, Sliding Window Attention Masks](https://www.tylerromero.com/assets/img/uDmVV2eE7Z-1273.svg)\n\nWhen combined with the `and_masks` function, these three masks Note that the causal mask is actually redundant to the sliding window mask, as the sliding window mask already ensures that tokens can only attend to previous tokens in the sequence. The causal mask is included here for clarity. work together to create an efficient attention pattern that respects document boundaries, maintains causality, and limits computational overhead for long sequences.\n\nAfter incorporating FlexAttention with these masks, and increasing our sequence length to 32768 tokens, we observe a massive speedup This speedup is a bit of a hack against the target metric. Supporting longer sequences is a straightforward way to drop the loss on the validation set, but is unlikely to provide a meaningful improvement to the overall performance of the model on practical benchmarks.. The new run time is **2.55 hours**, requiring only 1.88B tokens (a huge data-efficiency improvement). Our throughput dropped slightly to ~205k tokens/second. See commit [`d982ed5`](https://github.com/tyler-romero/nanogpt-speedrun/commit/d982ed5900922e43a266c5d671b88f36efe72aaf) for the full details.\n\n![Image 10: Section 3 loss plot](https://www.tylerromero.com/assets/img/PBvDQRx2tZ-300.png)\n\nReferences\n----------\n\n2024, Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, , Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and@Grad62304977[(view online)](https://github.com/KellerJordan/modded-nanogpt)\n\nmodded-nanogpt: Speedrunning the NanoGPT baseline\n\nhlb-gpt\n\n2023, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu[(view online)](https://arxiv.org/abs/2104.09864)\n\nRoFormer: Enhanced Transformer with Rotary Position Embedding\n\n2022, David R. So, Wojciech Ma\u0144ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le[(view online)](https://arxiv.org/abs/2109.08668)\n\nPrimer: Searching for Efficient Transformers for Language Modeling\n\n2024, Alexander H\u00e4gele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi[(view online)](https://arxiv.org/abs/2405.18392)\n\nScaling Laws and Compute-Optimal Training Beyond Fixed Training Durations\n\n2022, Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre[(view online)](https://arxiv.org/abs/2203.15556)\n\nTraining Compute-Optimal Large Language Models\n\n2024, Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein[(view online)](https://web.archive.org/web/20250122060345/https://kellerjordan.github.io/posts/muon/)\n\nMuon: An optimizer for hidden layers in neural networks\n\n2018, Vineet Gupta, Tomer Koren, and Yoram Singer[(view online)](https://arxiv.org/abs/1802.09568)\n\nShampoo: Preconditioned Stochastic Tensor Optimization\n\n2024, Jeremy Bernstein, and Laker Newhouse[(view online)](https://arxiv.org/abs/2409.20325)\n\nOld Optimizer, New Norm: An Anthology\n\n2024, Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi\u0144ska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci\u0144ska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin G\u00f6rner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, S\u00e9bastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev[(view online)](https://arxiv.org/abs/2408.00118)\n\nGemma 2: Improving Open Language Models at a Practical Size\n\n2024, Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He[(view online)](https://arxiv.org/abs/2412.05496)\n\nFlex Attention: A Programming Model for Generating Optimized Attention Kernels\n\nDeriving Muon\n",
        "timestamp": "2025-11-21T13:37:11.738655"
    },
    "https://github.com/KellerJordan/modded-nanogpt": {
        "content": "Title: GitHub - KellerJordan/modded-nanogpt: NanoGPT (124M) in 3 minutes\n\nURL Source: https://github.com/KellerJordan/modded-nanogpt\n\nMarkdown Content:\nModded-NanoGPT\n--------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#modded-nanogpt)\nThis repository hosts the _NanoGPT speedrun_, in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) validation set.\n\nThe target (3.28 validation loss on FineWeb) follows Andrej Karpathy's [GPT-2 replication in llm.c, which attains that loss after running for 45 minutes](https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29). The speedrun code also descends from llm.c's [PyTorch trainer](https://github.com/karpathy/llm.c/blob/master/train_gpt2.py), which itself descends from NanoGPT, hence the name of the repo. Thanks to the efforts of many contributors, this repo now contains a training algorithm which attains the target performance in:\n\n*   2 minutes and 20 seconds on 8xH100 (the llm.c GPT-2 replication needed 45)\n*   0.73B tokens (the llm.c GPT-2 replication needed 10B)\n\nThis improvement in training speed has been brought about by the following techniques:\n\n*   Modernized architecture: Rotary embeddings, QK-Norm, and ReLU\u00b2\n*   The Muon optimizer [[writeup](https://kellerjordan.github.io/posts/muon/)] [[repo](https://github.com/KellerJordan/Muon)]\n*   Untie head from embedding, use FP8 matmul for head, and softcap logits (the latter following Gemma 2)\n*   Initialization of projection and classification layers to zero (muP-like)\n*   Skip connections from embedding to every block as well as between blocks in U-net pattern\n*   Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)\n*   Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN\n*   Align training batch starts with EoS and set a max document length\n*   Accumulate gradients for 2 steps for embedding and lm_head before updating parameters\n*   Enable model to back out contributions from first 8 layers before prediction\n*   Polar Express implementation in Muon\n*   Smear module to enable 1 token look back\n*   Sparse attention gate\n\nAs well as many systems optimizations.\n\nContributors list (growing with each new record): [@bozavlado](https://x.com/bozavlado); [@brendanh0gan](https://x.com/brendanh0gan); [@fernbear.bsky.social](https://bsky.app/profile/fernbear.bsky.social); [@Grad62304977](https://x.com/Grad62304977); [@jxbz](https://x.com/jxbz); [@kellerjordan0](https://x.com/kellerjordan0); [@KoszarskyB](https://x.com/KoszarskyB); [@leloykun](https://x.com/@leloykun); [@YouJiacheng](https://x.com/YouJiacheng); [@jadenj3o](https://x.com/jadenj3o); [@KonstantinWilleke](https://github.com/KonstantinWilleke), [@alexrgilbert](https://github.com/alexrgilbert), [@adricarda](https://github.com/adricarda), [@tuttyfrutyee](https://github.com/tuttyfrutyee), [@vdlad](https://github.com/vdlad); [@ryanyang0](https://x.com/ryanyang0), [@vagrawal](https://github.com/vagrawal), [@classiclarryd](https://x.com/classiclarryd), [@byronxu99](https://github.com/byronxu99), [@varunneal](https://x.com/varunneal), [@EmelyanenkoK](https://github.com/EmelyanenkoK), [@bernard24](https://github.com/bernard24)/[https://www.hiverge.ai/](https://www.hiverge.ai/), [@GusarichOnX](https://x.com/GusarichOnX), [@li_zichong](https://x.com/li_zichong), [@akash5474](https://github.com/akash5474)\n\n* * *\n\nRunning the current record\n--------------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#running-the-current-record)\nTo run the current record, run the following commands.\n\ngit clone https://github.com/KellerJordan/modded-nanogpt.git && cd modded-nanogpt\npip install -r requirements.txt\npip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --upgrade\n# downloads only the first 900M training tokens to save time\npython data/cached_fineweb10B.py 9\n./run.sh\n\n**Note: torch.compile will add around 7 minutes of latency the first time you run the code.**\n\nAlternative: Running with Docker (recommended for precise timing)\n-----------------------------------------------------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#alternative-running-with-docker-recommended-for-precise-timing)\nFor cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).\n\ngit clone https://github.com/KellerJordan/modded-nanogpt.git && cd modded-nanogpt\nsudo docker build -t modded-nanogpt .\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh\n\nTo get an interactive docker, you can use\n\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash\n\n* * *\n\nWorld record history\n--------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#world-record-history)\nThe following is the historical progression of world speed records for the following competitive task:\n\n> _Train a neural network to \u22643.28 validation loss on FineWeb using 8x NVIDIA H100s._\n\nNote: The 3.28 target was selected to match [Andrej Karpathy's GPT-2 (small) reproduction](https://github.com/karpathy/llm.c/discussions/481).\n\n| # | Record time | Description | Date | Log | Contributors |\n| --- | --- | --- | --- | --- | --- |\n| 1 | 45 minutes | [llm.c baseline](https://github.com/karpathy/llm.c/discussions/481) | 05/28/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-13_llmc/main.log) | @karpathy, llm.c contributors |\n| 2 | 31.4 minutes | [Tuned learning rate & rotary embeddings](https://x.com/kellerjordan0/status/1798863559243513937) | 06/06/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-06-06_AdamW/f66d43d7-e449-4029-8adf-e8537bab49ea.log) | @kellerjordan0 |\n| 3 | 24.9 minutes | [Introduced the Muon optimizer](https://x.com/kellerjordan0/status/1842300916864844014) | 10/04/24 | none | @kellerjordan0, @jxbz |\n| 4 | 22.3 minutes | [Muon improvements](https://x.com/kellerjordan0/status/1844820919061287009) | 10/11/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-10_Muon/eb5659d0-fb6a-49e5-a311-f1f89412f726.txt) | @kellerjordan0, @bozavlado |\n| 5 | 15.2 minutes | [Pad embeddings, ReLU\u00b2, zero-init projections, QK-norm](https://x.com/kellerjordan0/status/1845865698532450646) | 10/14/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt) | @Grad62304977, @kellerjordan0 |\n| 6 | 13.1 minutes | [Distributed the overhead of Muon](https://x.com/kellerjordan0/status/1847291684016783746) | 10/18/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-17_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt) | @kellerjordan0 |\n| 7 | 12.0 minutes | [Upgraded PyTorch 2.5.0](https://x.com/kellerjordan0/status/1847358578686152764) | 10/18/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-18_PyTorch25/d4bfb25f-688d-4da5-8743-33926fad4842.txt) | @kellerjordan0 |\n| 8 | 10.8 minutes | [Untied embedding and head](https://x.com/kellerjordan0/status/1853188916704387239) | 11/03/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-03_UntieEmbed/d6b50d71-f419-4d26-bb39-a60d55ae7a04.txt) | @Grad62304977, @kellerjordan0 |\n| 9 | 8.2 minutes | [Value and embedding skip connections, momentum warmup, logit softcap](https://x.com/kellerjordan0/status/1854296101303800108) | 11/06/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-06_ShortcutsTweaks/dd7304a6-cc43-4d5e-adb8-c070111464a1.txt) | @Grad62304977, @kellerjordan0 |\n| 10 | 7.8 minutes | [Bfloat16 activations](https://x.com/kellerjordan0/status/1855267054774865980) | 11/08/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-08_CastBf16/a833bed8-2fa8-4cfe-af05-58c1cc48bc30.txt) | @kellerjordan0 |\n| 11 | 7.2 minutes | [U-net pattern skip connections & double lr](https://x.com/kellerjordan0/status/1856053121103093922) | 11/10/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-10_UNetDoubleLr/c87bb826-797b-4f37-98c7-d3a5dad2de74.txt) | @brendanh0gan |\n| 12 | 5.03 minutes | [1024-ctx dense causal attention \u2192 64K-ctx FlexAttention](https://x.com/kellerjordan0/status/1859331370268623321) | 11/19/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-19_FlexAttention/8384493d-dba9-4991-b16b-8696953f5e6d.txt) | @KoszarskyB |\n| 13 | 4.66 minutes | [Attention window warmup](https://x.com/hi_tysam/status/1860851011797053450) | 11/24/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-11-24_WindowWarmup/cf9e4571-c5fc-4323-abf3-a98d862ec6c8.txt) | @fernbear.bsky.social |\n| 14 | 4.41 minutes | [Value Embeddings](https://x.com/KoszarskyB/status/1864746625572257852) | 12/04/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-12-04_ValueEmbed) | @KoszarskyB |\n| 15 | 3.95 minutes | [U-net pattern value embeddings, assorted code optimizations](https://x.com/YouJiacheng/status/1865761473886347747) | 12/08/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-12-08_UNetValueEmbedsTweaks) | @leloykun, @YouJiacheng |\n| 16 | 3.80 minutes | [Split value embeddings, block sliding window, separate block mask](https://x.com/YouJiacheng/status/1866734331559071981) | 12/10/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-12-10_MFUTweaks) | @YouJiacheng |\n| 17 | 3.57 minutes | [Sparsify value embeddings, improve rotary embeddings, drop an attn layer](https://x.com/YouJiacheng/status/1868938024731787640) | 12/17/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-12-17_SparsifyEmbeds) | @YouJiacheng |\n| 18 | 3.4 minutes | [Lower logit softcap from 30 to 15](https://x.com/kellerjordan0/status/1876048851158880624) | 01/04/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-01-04_SoftCap/31d6c427-f1f7-4d8a-91be-a67b5dcd13fd.txt) | @KoszarskyB |\n| 19 | 3.142 minutes | [FP8 head, offset logits, lr decay to 0.1 instead of 0.0](https://x.com/YouJiacheng/status/1878827972519772241) | 01/13/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-01-13_Fp8LmHead/c51969c2-d04c-40a7-bcea-c092c3c2d11a.txt) | @YouJiacheng |\n| 20 | 2.992 minutes | [Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon](https://x.com/leloykun/status/1880301753213809016) | 01/16/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-01-16_Sub3Min/1d3bd93b-a69e-4118-aeb8-8184239d7566.txt) | @leloykun, @fernbear.bsky.social, @YouJiacheng, @brendanh0gan, @scottjmaddox, @Grad62304977 |\n| 21 | 2.933 minutes | [Reduced batch size](https://x.com/leloykun/status/1885640350368420160) | 01/26/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-01-26_BatchSize/c44090cc-1b99-4c95-8624-38fb4b5834f9.txt) | @leloykun |\n| 21 | 2.997 minutes | 21st record with new timing | 02/01/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-02-01_RuleTweak/eff63a8c-2f7e-4fc5-97ce-7f600dae0bc7.txt) | not a new record, just re-timing #21 with the [updated rules](https://github.com/KellerJordan/modded-nanogpt#timing-change-after-record-21) |\n| 21 | 3.014 minutes | 21st record with latest torch | 05/24/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-05-24_StableTorch/89d9f224-3b01-4581-966e-358d692335e0.txt) | not a new record, just re-timing #21 with latest torch |\n| 22 | 2.990 minutes | [Faster gradient all-reduce](https://x.com/KonstantinWille/status/1927137223238909969) | 05/24/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-05-24_FasterReduce/23f40b75-06fb-4c3f-87a8-743524769a35.txt) | @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad; The Enigma project |\n| 23 | 2.979 minutes | [Overlap computation and gradient communication](https://x.com/kellerjordan0/status/1927460573098262616) | 05/25/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-05-25_EvenFasterReduce/6ae86d05-5cb2-4e40-a512-63246fd08e45.txt) | @ryanyang0 |\n| 24 | 2.966 minutes | Replace gradient all_reduce with reduce_scatter | 05/30/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-05-30_noallreduce/8054c239-3a18-499e-b0c8-dbd27cb4b3ab.txt) | @vagrawal |\n| 25 | 2.896 minutes | Upgrade PyTorch to 2.9.0.dev20250713+cu126 | 07/13/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-07-13_UpgradeTorch190/692f80e0-5e64-4819-97d4-0dc83b7106b9.txt) | @kellerjordan0 |\n| 26 | 2.863 minutes | Align training batch starts with EoS, increase cooldown frac to .45 | 07/13/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-07-12_BosAlign/c1fd8a38-bb9f-45c4-8af0-d37f70c993f3.txt) | @classiclarryd |\n| 27 | 2.817 minutes | Transpose one of the MLP matrices + add Triton kernel for symmetric matmul | 07/18/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-07-18_TritonMuon/record.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/109) | @byronxu99 |\n| 28 | 2.812 minutes | Sparse attention gate | 08/23/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-08-23_SparseAttnGate/020630eb-2191-4ba2-9ee4-4cdc94316943.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/117) | @classiclarryd |\n| 29 | 2.731 minutes | Flash Attention 3, 2048 max_doc_len, update ws schedule | 09/03/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-03_FA3/44fc1276-0510-4961-92c0-730c65e5feba.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/118) | @varunneal |\n| 30 | 2.717 minutes | Drop first MLP layer | 09/05/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-05_SkipMLPBlocks/07e7ae76-b7d0-4481-b149-01e7d81b5ad4.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/120) | @EmelyanenkoK |\n| 31 | 2.656 minutes | Dynamically incorporate YaRN during training and validation | 09/10/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-10_Yarn/0ecdb695-510b-4c3b-b030-09861a162ce8.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/122) | @classiclarryd |\n| 32 | 2.625 minutes | Optimize distributed training, improve skip connection gating, and enhance bfloat16 usage | 09/11/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-11_VectSigmoidBFloat16/0d0d9882-c34f-4d82-b961-a17d5659c988.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/125) | @bernard24 & hiverge.ai |\n| 33 | 2.565 minutes | Asynchronously fetch and index data batches, extend final layer attention window for validation | 09/15/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-15_AsyncDataLoadAttnFinalWindow/25db37c7-2bab-4ef4-ae63-d593590ef823.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/127) | @classiclarryd |\n| 34 | 2.547 minutes | Smear token embeddings 1 position forward | 09/18/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-18_Smear/18a1e5c7-947e-479d-bc3a-a57a61a98fc9.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/130) | @classiclarryd |\n| 35 | 2.527 minutes | Drop first attn layer, extend all long windows for validation, update schedule | 09/21/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-21_DropAttn/01fc4a96-f2a0-47a1-8a6a-c7d10bac99fe.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/131) | @classiclarryd |\n| 36 | 2.495 minutes | MuonCustomSizing, perform mlp and attn reduce scatter in shared call | 09/23/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-23_MuonCustomSizing/b067b4ac-72a6-4436-a6f8-ea51c1efeef3.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/132) | @classiclarryd |\n| 37 | 2.483 minutes | Compute cross entropy in BF16 during training | 09/27/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-27_BF16CE/08c0770f-17fc-44cd-971d-734a7a28a3e3.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/133) | @GusarichOnX |\n| 38 | 2.476 minutes | Polar Express, replacement for Newton-Schulz | 09/29/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-29_PolarExpress/0e3f0af5-ad08-47a6-813d-0c709b50d422.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/134) | @varunneal |\n| 39 | 2.447 minutes | Only update Adam params every other step, reduce batch size | 09/30/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-09-30_CustomBatching/40b101b1-77ea-45ea-a089-1d3a647daa22.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/136) | @classiclarryd |\n| 40 | 2.358 minutes | Backout, misc hyperparameter tuning, optimize lambda padding | 10/04/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-10-04_Backout/514e7581-fbd4-4338-a3e4-e556f9c958ce.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/140) | @classiclarryd |\n| 41 | 2.345 minutes | [NorMuon](https://arxiv.org/pdf/2510.05491) | 10/24/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-10-24_NorMuon/088a77ee-9b67-475a-bbb9-3e92e4698799.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/144) | @li_zichong |\n| 42 | 2.313 minutes | Update NorMuon LR, Step Logic | 10/27/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-10-27_FixMuonLR/14afd380-d3d9-48d7-ad23-4c13cb96754b.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/146) | @varunneal |\n| 43 | 2.284 minutes | Cautious Weight Decay w/ Schedule | 11/10/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-11-10_CautiousWD/1aac0132-a891-4ed9-b358-0fd2abd1b019.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/154) | @varunneal |\n| 44 | 2.269 minutes | Backward Hooks on Adam, [Profiling 101](https://blog.underfit.ai/profiling-101-nanogpt) | 11/16/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-10-31_AdamSyncGradientHook/0c17cdfd-772c-4906-8d11-141b370599a0.txt),[PR](https://github.com/KellerJordan/modded-nanogpt/pull/149) | @akash5474 |\n\nRules\n-----\n\n[](https://github.com/KellerJordan/modded-nanogpt#rules)\nThe only rules are that new records must:\n\n1.   Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)\n2.   Attain \u22643.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p<0.01 that their mean val loss is \u22643.28. Example code to compute p-value can be found [here](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap). For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)\n3.   Not use any extra `torch._inductor.config` or `torch.compile` flags. (These can save a few seconds, but they can also make compilation take >30min. This rule was introduced after the 21st record.)\n\n> Note: `torch._inductor.config.coordinate_descent_tuning` is allowed for GPT-2 Medium track (a.k.a. 2.92 track).\n\nOther than that, anything and everything is fair game!\n\n[further clarifications](https://github.com/KellerJordan/modded-nanogpt/discussions/23?sort=new#discussioncomment-12109560)\n\n* * *\n\n### Comment on the target metric\n\n[](https://github.com/KellerJordan/modded-nanogpt#comment-on-the-target-metric)\nThe target metric is _cross-entropy loss on the FineWeb val set_. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least `math.exp(-3.28 * 10485760)` to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.\n\n* * *\n\n### Timing change after record 21\n\n[](https://github.com/KellerJordan/modded-nanogpt#timing-change-after-record-21)\nAfter the 21st record, we made two changes to the timing. First, there used to be an initial \"grace period\" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of `torch._inductor.config.coordinate_descent_tuning`. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.\n\n* * *\n\n### Important note about records 22-25\n\n[](https://github.com/KellerJordan/modded-nanogpt#important-note-about-records-22-25)\nThanks to the statistical testing of [@agrawal](https://www.github.com/agrawal) (holder of the 24th record), we have learned that records 23, 24, and in all likelihood 22 and 25, actually attain a mean loss of 3.281, which is slightly above the 3.28 target. Therefore if we were to completely adhere to the speedrun rules, we would have to deny that these are valid records. However, we have decided to leave them in place as valid, because of the following two reasons: (a) the extra loss is most likely my (@kellerjordan0) own fault rather than that of the records, and (b) it is most likely easily addressable.\n\nHere's what happened: Records #22 to #25 each change only the systems/implementation of the speedrun. Therefore, the requirement to do statistical testing to confirm they hit the target was waived, since in theory they should have hit it automatically, by virtue of the fact that they didn't touch the ML (i.e., they didn't change the architecture, learning rate, etc.).\n\nSo if these records shouldn't have changed the ML, what explains the regression in val loss? We think that most likely, the answer is that this regression was indeed not introduced by any of these records. Instead, it was probably caused by my own non-record in which I retimed record #21 with newest torch, because in this non-record I also changed the constants used to cast the lm_head to fp8. I thought that this change should cause only a (small) strict improvement, but apparently that was not the case.\n\nTherefore, it is probable that each of records #22-25 could be easily made fully valid by simply reverting the change I made to those constants. Therefore they shall be upheld as valid records.\n\nFor the future, fortunately record #26 brought the speedrun back into the green in terms of <3.28 loss, so (with high p-value) it should be in a good state now.\n\n* * *\n\n### Notable attempts & forks\n\n[](https://github.com/KellerJordan/modded-nanogpt#notable-attempts--forks)\n**Notable runs:**\n\n*   [@alexjc's 01/20/2025 2.77-minute TokenMonster-based record](https://x.com/alexjc/status/1881410039639863622). This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.\n\n**Notable forks:**\n\n*   [https://github.com/BlinkDL/modded-nanogpt-rwkv](https://github.com/BlinkDL/modded-nanogpt-rwkv)\n*   [https://github.com/nikhilvyas/modded-nanogpt-SOAP](https://github.com/nikhilvyas/modded-nanogpt-SOAP)\n\n* * *\n\nSpeedrun track 2: GPT-2 Medium\n------------------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#speedrun-track-2-gpt-2-medium)\nThe target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.\n\n> Note: `torch._inductor.config.coordinate_descent_tuning` is turned on after the record 6 (*).\n\n| # | Record time | Description | Date | Log | Contributors |\n| --- | --- | --- | --- | --- | --- |\n| 1 | 5.8 hours | [llm.c baseline (350M parameters)](https://github.com/karpathy/llm.c/discussions/481) | 05/28/24 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-01-18/main.log) | @karpathy, llm.c contributors |\n| 2 | 29.3 minutes | [Initial record based on scaling up the GPT-2 small track speedrun](https://x.com/kellerjordan0/status/1881959719012847703) | 01/18/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-01-18/241dd7a7-3d76-4dce-85a4-7df60387f32a.txt) | @kellerjordan0 |\n| 3 | 28.1 minutes | [Added standard weight decay](https://x.com/kellerjordan0/status/1888320690543284449) | 02/08/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-02-08_WeightDecay/b01743db-605c-4326-b5b1-d388ee5bebc5.txt) | @kellerjordan0 |\n| 4 | 27.7 minutes | [Tuned Muon Newton-Schulz coefficients](https://x.com/leloykun/status/1892793848163946799) | 02/14/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-02-14_OptCoeffs/1baa66b2-bff7-4850-aced-d63885ffb4b6.txt) | @leloykun |\n| 5 | 27.2 minutes | [Increased learning rate cooldown phase duration](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt) | 03/06/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt) | @YouJiacheng |\n| 6 | 25.95 minutes* | [2x MLP wd, qkv norm, all_reduce/opt.step() overlap, optimized skip pattern](https://x.com/YouJiacheng/status/1905861218138804534) | 03/25/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-03-25_ArchOptTweaks/train_gpt-20250329.txt) | @YouJiacheng |\n| 7 | 25.29 minutes | [Remove FP8 head; ISRU logits softcap; New sharded mixed precision Muon; merge weights](https://x.com/YouJiacheng/status/1912570883878842527) | 04/16/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-04-16_Record7/223_3310d0b1-b24d-48ee-899f-d5c2a254a195.txt) | @YouJiacheng |\n| 8 | 24.50 minutes | [Cubic sliding window size schedule, 2\u00d7 max window size (24.84 minutes)](https://x.com/jadenj3o/status/1914893086276169754)[24.5min repro](https://x.com/YouJiacheng/status/1915667616913645985) | 04/22/25 | [log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_2_medium/2025-04-22_Record8/075_640429f2-e726-4e83-aa27-684626239ffc.txt) | @jadenj3o |\n\n* * *\n\n### Q: What is the point of NanoGPT speedrunning?\n\n[](https://github.com/KellerJordan/modded-nanogpt#q-what-is-the-point-of-nanogpt-speedrunning)\nA: The officially stated goal of NanoGPT speedrunning is as follows: `gotta go fast`. But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. [https://x.com/karpathy/status/1846790537262571739](https://x.com/karpathy/status/1846790537262571739)\n\n### Q: What makes \"NanoGPT speedrunning\" not just another idiosyncratic benchmark?\n\n[](https://github.com/KellerJordan/modded-nanogpt#q-what-makes-nanogpt-speedrunning-not-just-another-idiosyncratic-benchmark)\nA: Because it is a _competitive_ benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.\n\n[\"Artificial intelligence advances by inventing games and gloating to goad others to play\" - Professor Ben Recht](https://www.argmin.net/p/too-much-information)\n\n### Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss\n\n[](https://github.com/KellerJordan/modded-nanogpt#q-nanogpt-speedrunning-is-cool-and-all-but-meh-it-probably-wont-scale-and-is-just-overfitting-to-val-loss)\nA: This is hard to refute, since \"at scale\" is an infinite category (what if the methods stop working only for >100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which _impose additional structure_ on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:\n\n_Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than [@karpathy's baseline](https://github.com/karpathy/llm.c/discussions/677) ($233 instead of $576):_\n\n[![Image 1](https://github.com/KellerJordan/modded-nanogpt/raw/master/img/nanogpt_speedrun51.png)](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/nanogpt_speedrun51.png) [[reproducible log](https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-20_ScaleUp1B/ad8d7ae5-7b2d-4ee9-bc52-f912e9174d7a.txt)] [![Image 2](https://github.com/KellerJordan/modded-nanogpt/raw/master/img/nanogpt_speedrun52.png)](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/nanogpt_speedrun52.png)\n\n* * *\n\n[Muon optimizer](https://github.com/KellerJordan/Muon)\n------------------------------------------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#muon-optimizer)\nMuon is defined as follows:\n\n[![Image 3](https://github.com/KellerJordan/modded-nanogpt/raw/master/img/algo_optimizer.png)](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/algo_optimizer.png)\n\nWhere NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces `G` with `U @ V.T` where `U, S, V = G.svd()`.\n\n@torch.compile\ndef zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):\n    assert len(G.shape) == 2\n    a, b, c = (3.4445, -4.7750,  2.0315)\n    X = G.bfloat16() / (G.norm() + eps)\n    if G.size(0) > G.size(1):\n        X = X.T \n    for _ in range(steps):\n        A = X @ X.T\n        B = b * A + c * A @ A\n        X = a * X + B @ X\n    if G.size(0) > G.size(1):\n        X = X.T \n    return X.to(G.dtype)\n\nFor this training scenario, Muon has the following favorable properties:\n\n*   Lower memory usage than Adam\n*   ~1.5x better sample-efficiency\n*   <2% wallclock overhead\n\n### Provenance\n\n[](https://github.com/KellerJordan/modded-nanogpt#provenance)\nMany of the choices made to generate this optimizer were obtained experimentally by our pursuit of [CIFAR-10 speedrunning](https://github.com/KellerJordan/cifar10-airbench). In particular, we experimentally obtained the following practices:\n\n*   Using Nesterov momentum inside the update, with orthogonalization applied after momentum.\n*   Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.\n*   Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.\n*   Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).\n\nOur use of a Newton-Schulz iteration for orthogonalization traces to [Bernstein & Newhouse (2024)](https://arxiv.org/abs/2409.20325), who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein & Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.\n\n* * *\n\nRunning on fewer GPUs\n---------------------\n\n[](https://github.com/KellerJordan/modded-nanogpt#running-on-fewer-gpus)\n*   To run experiments on fewer GPUs, simply modify `run.sh` to have a different `--nproc_per_node`. This should not change the behavior of the training.\n*   If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see [here](https://github.com/KellerJordan/modded-nanogpt/pull/38) for a guide)\n\n* * *\n\nReferences\n----------\n\n[](https://github.com/KellerJordan/modded-nanogpt#references)\n1.   [Guilherme Penedo et al. \"The fineweb datasets: Decanting the web for the finest text data at scale.\" arXiv preprint arXiv:2406.17557 (2024).](https://arxiv.org/abs/2406.17557)\n2.   Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics (2008). Equation 5.22.\n3.   G\u00c3\u00bcnther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57\u00e2\ufffd\ufffd59 (1933).\n4.   [Jeremy Bernstein and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" arxiv preprint arXiv:2409.20325 (2024).](https://arxiv.org/abs/2409.20325)\n5.   [Vineet Gupta, Tomer Koren, and Yoram Singer. \"Shampoo: Preconditioned stochastic tensor optimization.\" International Conference on Machine Learning. PMLR, 2018.](https://arxiv.org/abs/1802.09568)\n6.   [Rohan Anil et al. \"Scalable second order optimization for deep learning.\" arXiv preprint arXiv:2002.09018 (2020).](https://arxiv.org/abs/2002.09018)\n7.   [Alexander H\u00c3\u00a4gele et al. \"Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations.\" arXiv preprint arXiv:2405.18392 (2024).](https://arxiv.org/abs/2405.18392)\n8.   [Zhanchao Zhou et al. \"Value Residual Learning For Alleviating Attention Concentration In Transformers.\" arXiv preprint arXiv:2410.17897 (2024).](https://arxiv.org/abs/2410.17897)\n9.   [Team, Gemma, et al. \"Gemma 2: Improving open language models at a practical size.\" arXiv preprint arXiv:2408.00118 (2024).](https://arxiv.org/abs/2408.00118)\n10.   [Alec Radford et al. \"Language models are unsupervised multitask learners.\" OpenAI blog 1.8 (2019).](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n\nCitation\n--------\n\n[](https://github.com/KellerJordan/modded-nanogpt#citation)\n\n```\n@misc{modded_nanogpt_2024,\n  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and\n                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and\n                  Franz Cesista and Braden Koszarsky and @Grad62304977},\n  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},\n  year         = {2024},\n  url          = {https://github.com/KellerJordan/modded-nanogpt}\n}\n```\n\n[![Image 4: itsover_wereback](https://github.com/KellerJordan/modded-nanogpt/raw/master/img/dofa.jpg)](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/dofa.jpg)\n",
        "timestamp": "2025-11-21T13:37:11.114492"
    }
}